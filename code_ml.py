# -*- coding: utf-8 -*-
"""code_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/wahidulalamriyad/Multitask_Classification_On_Amazon_Reviews/blob/main/code_ML.ipynb

#**Text Classification: Amazon Reviews**

This task is to implement a ML solution for a multi-task classification problem from text data (mostly short texts). Specifically, you are provided with Amazon reviews (the text is the review title and the review main body joined together) and your task is to predict the following attributes:

* The number of stars associated with the review (on a scale of 1 to 5).
* Whether a product is from the category “Video Games” (“video_games”) or “Musical Instrument” (“musical_instrument”).

Note that for the first attribute you can choose whether to use multiclass classification or regression. You can choose whether to predict both features simultaneously or separately. Additionally, the dataset is provided with a further attribute: whether the review is verified or not (either True or False), which is optional to use. If you want to explore the data further, a separate dataset with the text field split into the original fields “review title”, “review main body” can be provided upon request.


---

### **Importing and installing the required libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install contractions
!pip install textsearch
!pip install tqdm

#python modules
import string
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import pandas as pd
from sklearn import model_selection
import seaborn as sns
import re
import contractions
from bs4 import BeautifulSoup
import tqdm
import unicodedata

#nltk libraries
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk import word_tokenize
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize

#sklearn libraries
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression,SGDClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics 
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer,TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier
from sklearn.svm import LinearSVC
from sklearn.model_selection import GridSearchCV

#Tensorflow layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Conv1D
from tensorflow.keras.layers import MaxPooling1D
from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing import sequence
from sklearn.preprocessing import LabelEncoder

#fix random seed for reproducibility
seed = 42
np.random.seed(seed)

"""## **1.   Exploratory Data Analysis**

### **Importing Amazon Review Dataset**
"""

#Fetching Amazon review dataset and reviewing the data types
df = pd.read_csv('processed_reviews_split_surnamesR_minimal.csv')
df

df.info()

df.shape

#Creating multiple columns to monitior all the null values and duplicated values along with their reason of exclusion

df['excluded'] = df.isna().sum(axis=1).apply(lambda x: 1 if x == 0 else 0)

df['reason_for_exclusion'] = df.isnull().sum(axis=1).apply(lambda x: 'Missing values' if x == 0  else 'N/A')

m1 =  df.duplicated(['text'])   

df['reason_for_exclusion'] = np.select([m1],['duplicated'], default='N/A')

df.insert(len(df.columns)-6, 'excluded', df.pop('excluded'))

df.insert(len(df.columns)-5, 'reason_for_exclusion', df.pop('reason_for_exclusion'))

df

df['reason_for_exclusion'].unique()

#Dowloading the datset with null value and duplicated 
df.to_csv('exclusions_dataset_task4.csv')

#Creating a new dataframe with the data required for classification
dataset = df[['text','review_score','product_category']]

dataset

"""## **2.   Data Pre-Processing**"""

#Checking for Null Values
dataset.isnull().sum()

#Checking for any duplicates in the text columns
dataset.duplicated().sum()

#Removing the Duplicates in text column
dataset=dataset.drop_duplicates('text', keep='first')

#Removing Null values in the dataset
dataset.dropna(inplace=True)
dataset.shape

dataset.isnull().sum()

dataset['review_score'].value_counts()

dataset['review_score'] = dataset['review_score'].replace([-1.0],1.0).astype(int)

dataset['review_score'].unique()

"""## **3.   Data Splitting**"""

#Build train and test datasets
text = dataset['text'].values
category = dataset['product_category'].values

train_text = text[:20800]
train_category = category[:20800]

test_text = text[20800:]
test_category = category[20800:]

"""### **3.1   Text Wrangling & Normalisation - Text Processing**"""

def strip_html_tags(text):
  soup = BeautifulSoup(text, "html.parser")
  [s.extract() for s in soup(['iframe', 'script'])]
  stripped_text = soup.get_text()
  stripped_text = re.sub(r'[\r|\n|\r\n]+', '\n', stripped_text)
  return stripped_text

def remove_accented_chars(text):
  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
  return text

def pre_process_corpus(docs):
  norm_docs = []
  for doc in tqdm.tqdm(docs):
    doc = strip_html_tags(doc)
    doc = doc.translate(doc.maketrans("\n\t\r", "   "))
    doc = doc.lower()
    doc = remove_accented_chars(doc)
    doc = contractions.fix(doc)
    # lower case and remove special characters\whitespaces
    doc = re.sub(r'[^a-zA-Z0-9\s]', '', doc, re.I|re.A)
    doc = re.sub(' +', ' ', doc)
    doc = doc.strip()  
    norm_docs.append(doc)
  
  return norm_docs

# Commented out IPython magic to ensure Python compatibility.
# %%time
# dataset['cleantext'] = pre_process_corpus(dataset['text'])
# dataset.head()

#Build train and test datasets
reviews = dataset['cleantext'].values
category = dataset['product_category'].values

train_reviews = reviews[:4300]
train_category = category[:4300]

test_reviews = reviews[4300:]
test_category = category[4300:]

#After preprocesing
dataset.isnull().sum()

"""### **3.2   Splitting the dataset into Train and Test Sets**"""

t = Tokenizer(oov_token='<UNK>')
#fit the tokenizer on the documents
t.fit_on_texts(train_reviews)
t.word_index['<PAD>'] = 0

max([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), min([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), t.word_index['<UNK>']

train_sequences = t.texts_to_sequences(train_reviews)

test_sequences = t.texts_to_sequences(test_reviews)

print("Vocabulary size={}".format(len(t.word_index)))
print("Number of Documents={}".format(t.document_count))

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

train_lens = [len(s) for s in train_sequences]
test_lens = [len(s) for s in test_sequences]

fig, ax = plt.subplots(1,2, figsize=(12, 6))
h1 = ax[0].hist(train_lens)
h2 = ax[1].hist(test_lens)

"""### **3.3   Sequencing Normalisation**"""

MAX_SEQUENCE_LENGTH = 1800

#Pad dataset to a maximum review length in words
X_train = sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)
X_test = sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)
X_train.shape, X_test.shape

"""### **3.4   Encoding Labels**"""

le = LabelEncoder()
num_classes=2 # “video_games” -> 1, “musical_instrument” -> 0

y_train = le.fit_transform(train_category)
y_test = le.transform(test_category)

VOCAB_SIZE = len(t.word_index)

"""## **4.   Preparing the Model**

Since textual data is a sequence of words, we utilize 1D convolutions to scan through the sentences. The model first transforms each word into lower dimensional embedding/vector space followed by 1d convolutions and then passing the data through dense layers before the final layer for classification.
"""

EMBED_SIZE = 300
EPOCHS = 2
BATCH_SIZE = 128

#Creating the model
model = Sequential()
model.add(Embedding(VOCAB_SIZE, EMBED_SIZE, input_length=MAX_SEQUENCE_LENGTH))
model.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(filters=64, kernel_size=4, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(filters=32, kernel_size=4, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

"""### **4.1   Model Training**"""

#Fitting the model
model.fit(X_train, y_train, 
          validation_split=0.3,
          epochs=EPOCHS, 
          batch_size=BATCH_SIZE, 
          verbose=1)

"""### **4.2   Model Evaluation**"""

#Final evaluation of the model
scores = model.evaluate(X_test, y_test, verbose=1)
print("Accuracy: %.2f%%" % (scores[1]*100))

predictions = (model.predict(X_test) > 0.5).astype("int32").ravel()
predictions[:10]

from sklearn.metrics import confusion_matrix, classification_report

labels = ['musical_instrument', 'video_games']
print(classification_report(y_test, predictions))
#Confusion matrix
cm = confusion_matrix(y_test, predictions)
pd.DataFrame(confusion_matrix(y_test, predictions), index=labels, columns=labels)

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sn
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline

#plt.figure(figsize = (10,7))
#sn.heatmap(cm, annot=True);
plt.clf()
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)
classNames = ['Musical Instrument','Video Games']
plt.title('Confusion Matrix')
plt.ylabel('True label')
plt.xlabel('Predicted label')
tick_marks = np.arange(len(classNames))
plt.xticks(tick_marks, classNames, rotation=45)
plt.yticks(tick_marks, classNames)
s = [['TN','FP'], ['FN', 'TP']]
for i in range(2):
    for j in range(2):
        plt.text(j,i, str(s[i][j])+" = "+str(cm[i][j]))
plt.show()

#Validation
df_test= pd.DataFrame(df.sample(n=5), columns=['text','product_category'])
df_test

df_test_1_reviews = pre_process_corpus(df_test['text']) #preprocess the data
t.fit_on_texts(df_test_1_reviews)
df_test_1_train_sequences = t.texts_to_sequences(df_test_1_reviews)
df_test_1_X_train = sequence.pad_sequences(df_test_1_train_sequences, maxlen=MAX_SEQUENCE_LENGTH)
df_test_1 = (model.predict(df_test_1_X_train) > 0.5).astype("int32").ravel()
df_test_1_pred = ['musical_instruments' if item == 1 else 'video_games' for item in df_test_1]

df_test['predicted_category'] = df_test_1_pred
df_test

"""## **5.   Multiclassification of text using Tfid Vectorisation**"""

#Checking the target distribution
import seaborn as sns
x=dataset['review_score'].value_counts()
sns.barplot(x.index,x)

#Assign stopwords 
stopwords_list = nltk.corpus.stopwords.words('english')
stopwords_list

#Preprocess clean text
stop = stopwords.words('english')
stop.append('I')
stop_words = []
dataset['cleantext'] = dataset['cleantext'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
dataset.head()

#Lemmatisation
from nltk.stem import WordNetLemmatizer 
from nltk.tokenize import word_tokenize 
lemmatizer = WordNetLemmatizer() 

#Lemmatise string 
def lemmatize_word(text): 
    word_tokens = word_tokenize(text) 
    # provide context i.e. part-of-speech 
    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens] 
    
    return lemmas 

lemmatized_words = dataset.apply(lambda row: lemmatize_word(row['cleantext']),axis=1) 
#text = 'Fun if you havent played any of the games before!. Fun!'
lemmatized_words

#Tokenisation
tokenized_words = dataset.apply(lambda row: word_tokenize(row['cleantext']),axis=1)
tokenized_words

"""### **5.1   Spliting the data into Train and Test Sets**"""

#Vectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split

vectorizer = TfidfVectorizer(max_features = 30000, ngram_range = (1,3), analyzer = 'char')
X = vectorizer.fit_transform(dataset['cleantext'])
y = dataset['review_score']

X_train,X_test, y_train, y_test = train_test_split(X,y,test_size = 0.30, random_state = 50)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""## **6.   Implementing numerous Models to Predict Accuracy**

### **6.1   Support Vector Machines**
"""

#Linear SVC
lsv = LinearSVC()
lsv.fit(X_train,y_train)

from sklearn.metrics import classification_report
lsv_predict = lsv.predict(X_test)
lsv_predict

#Evaluation

print(classification_report(y_test,lsv_predict))
print(metrics.accuracy_score(y_test,lsv_predict))
print(confusion_matrix(y_test,lsv_predict))

"""### **6.2   Logistic Regression**"""

#LRclassification
lr = LogisticRegression()
lr.fit(X_train,y_train)
lr_predict = lr.predict(X_test)
lr_predict

#Evaluation
print(classification_report(y_test,lr_predict))
print(metrics.accuracy_score(y_test,lr_predict))
print(confusion_matrix(y_test,lr_predict))

"""### **6.3   Naive Bayes**"""

#Naive Bayes
nbm = MultinomialNB()
nbm.fit(X_train,y_train)
nbm_predict = nbm.predict(X_test)
nbm_predict

#Evaluation

print(classification_report(y_test,nbm_predict))
print(metrics.accuracy_score(y_test,nbm_predict))
print(confusion_matrix(y_test,nbm_predict))

"""### **6.4   Random Forest**"""

rfr = RandomForestClassifier()
rfr.fit(X_train,y_train)
rfr_predict = rfr.predict(X_test)
rfr_predict

#Evaluation

print(classification_report(y_test,rfr_predict))
print(metrics.accuracy_score(y_test,rfr_predict))
print(confusion_matrix(y_test,rfr_predict))

"""## **7.   Best Model**"""

svm_acc = accuracy_score(y_test,lsv_predict)
log_acc = accuracy_score(y_test,lr_predict)
nb_acc = accuracy_score(y_test,nbm_predict)
rf_acc = accuracy_score(y_test,rfr_predict)

models = pd.DataFrame({
                      'Model': ['Logistic Regression', 'SVM', 'Naive Bayes', 'Random Forest'],
                      'Score': [log_acc, svm_acc, nb_acc, rf_acc]})
models.sort_values(by='Score', ascending=False)